{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Visual Transformer from Scratch - PaliGemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Siglip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiglipVisionConfig:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size = 768,\n",
    "        intermediate_size = 3072,\n",
    "        num_hidden_layers = 12,\n",
    "        num_attention_heads = 12,\n",
    "        num_channels = 3,\n",
    "        image_size = 224,\n",
    "        patch_size = 16,\n",
    "        layer_norm_eps = 1e-6,\n",
    "        attention_dropout = 0.0,\n",
    "        num_image_tokens : int = None,\n",
    "        **kwargs\n",
    "        ):\n",
    "            super().__init__()\n",
    "            self.hidden_size = hidden_size\n",
    "            self.intermediate_size = intermediate_size\n",
    "            self.num_hidden_layers = num_hidden_layers\n",
    "            self.num_attention_heads = num_attention_heads\n",
    "            self.num_channels = num_channels\n",
    "            self.patch_size = patch_size\n",
    "            self.image_size = image_size\n",
    "            self.attention_dropout = attention_dropout\n",
    "            self.layer_norm_eps = layer_norm_eps\n",
    "            self.num_image_tokens = num_image_tokens\n",
    "\n",
    "class SiglipVisionEmbeddings(nn.Module):\n",
    "    def __init__(self, config: SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.image_size = config.image_size\n",
    "        self.patch_size = config.patch_size\n",
    "\n",
    "        self.patch_embedding = nn.Conv2d(\n",
    "            in_channels=config.num_channels,\n",
    "            out_channels=self.embed_dim,\n",
    "            kernel_size=self.patch_size,\n",
    "            stride=self.patch_size,\n",
    "            padding=\"valid\" # Indicates no padding\n",
    "        )\n",
    "\n",
    "        self.num_patches = (self.image_size // self.patch_size) ** 2 \n",
    "        self.num_positions = self.num_patches\n",
    "        self.num_position_embedding = nn.Embedding(self.num_positions, self.embed_dim)\n",
    "        self.register_buffer(\n",
    "            \"position_ids\",\n",
    "            torch.arange(self.num_positions).expand((1,-1)),\n",
    "            persistent=False\n",
    "        )\n",
    "\n",
    "    def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:\n",
    "        _, _, height, width = pixel_values.shape # [Batch_Size, Channels, Height, Width]\n",
    "        # Convolve the `patch_size` kernel over the image, with no overlapping patches since the stride is equal to the kernel size\n",
    "        # The output of the convolution will have shape [Batch_Size, Embed_Dim, Num_Patches_H, Num_Patches_W]\n",
    "        # where Num_Patches_H = height // patch_size and Num_Patches_W = width // patch_size\n",
    "        patch_embeds = self.patch_embedding(pixel_values)  \n",
    "        # [Batch_Size, Embed_Dim, Num_Patches_H, Num_Patches_W] -> [Batch_Size, Embed_Dim, Num_Patches]\n",
    "        # where Num_Patches = Num_Patches_H * Num_Patches_W\n",
    "        embeddings = patch_embeds.flatten(2)\n",
    "        # [Batch_Size, Embed_Dim, Num_Patches] -> [Batch_Size, Num_Patches, Embed_Dim]\n",
    "        embeddings = embeddings.transpose(1, 2)\n",
    "        # Add position embeddings to each patch. Each positional encoding is a vector of size [Embed_Dim]\n",
    "        embeddings = embeddings + self.position_embedding(self.position_ids)\n",
    "        # [Batch_Size, Num_Patches, Embed_Dim]\n",
    "        return embeddings\n",
    "\n",
    "class SiglipAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        self.scale = self.head_dim**-0.5 # Equivalent to 1 / sqrt(self.head_dim)\n",
    "        self.dropout = config.attention_dropout\n",
    "\n",
    "        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "\n",
    "    def forward(self,hidden_states: torch.Tensor,) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "\n",
    "        # hidden_states: [Batch_Size, Num_Patches, Embed_Dim]\n",
    "        batch_size, seq_len, _ = hidden_states.size()\n",
    "        # query_states: [Batch_Size, Num_Patches, Embed_Dim]\n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        # key_states: [Batch_Size, Num_Patches, Embed_Dim]\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        # value_states: [Batch_Size, Num_Patches, Embed_Dim]\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "        # query_states: [Batch_Size, Num_Heads, Num_Patches, Head_Dim]\n",
    "        query_states = query_states.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        key_states = key_states.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        value_states = value_states.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        # Calculate the attention using the formula Q * K^T / sqrt(d_k). attn_weights: [Batch_Size, Num_Heads, Num_Patches, Num_Patches]\n",
    "        attn_weights = (torch.matmul(query_states, key_states.transpose(2, 3)) * self.scale)\n",
    "\n",
    "        if attn_weights.size() != (batch_size, self.num_heads, seq_len, seq_len):\n",
    "            raise ValueError(\n",
    "                f\"Attention weights should be of size {(batch_size, self.num_heads, seq_len, seq_len)}, but is\"\n",
    "                f\" {attn_weights.size()}\"\n",
    "            )\n",
    "\n",
    "        # Apply the softmax row-wise. attn_weights: [Batch_Size, Num_Heads, Num_Patches, Num_Patches]\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "        # Apply dropout only during training\n",
    "        attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
    "        # Multiply the attention weights by the value states. attn_output: [Batch_Size, Num_Heads, Num_Patches, Head_Dim]\n",
    "        attn_output = torch.matmul(attn_weights, value_states)\n",
    "\n",
    "        if attn_output.size() != (batch_size, self.num_heads, seq_len, self.head_dim):\n",
    "            raise ValueError(\n",
    "                f\"`attn_output` should be of size {(batch_size, self.num_heads, seq_len, self.head_dim)}, but is\"\n",
    "                f\" {attn_output.size()}\"\n",
    "            )\n",
    "        # [Batch_Size, Num_Heads, Num_Patches, Head_Dim] -> [Batch_Size, Num_Patches, Num_Heads, Head_Dim]\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        # [Batch_Size, Num_Patches, Num_Heads, Head_Dim] -> [Batch_Size, Num_Patches, Embed_Dim]\n",
    "        attn_output = attn_output.reshape(batch_size, seq_len, self.embed_dim)\n",
    "        # [Batch_Size, Num_Patches, Embed_Dim]\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "\n",
    "        return attn_output, attn_weights\n",
    "\n",
    "class SiglipMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "    \n",
    "    def forward(self, hidden_states:torch.Tensor) -> torch.Tensor :\n",
    "        # [Batch_Size, Num_Patches, Embed_Dim] -> [Batch_Size, Num_Patches, Intermediate_Size]\n",
    "        hidden_states = self.fc1(hidden_states)\n",
    "        # hidden_states: [Batch_Size, Num_Patches, Intermediate_Size]\n",
    "        hidden_states = nn.functional.gelu(hidden_states, approximate=\"tanh\")\n",
    "        # [Batch_Size, Num_Patches, Intermediate_Size] -> [Batch_Size, Num_Patches, Embed_Dim]\n",
    "        hidden_states = self.fc2(hidden_states)\n",
    "\n",
    "        return hidden_states       \n",
    "\n",
    "class SiglipEncoderLayer(nn.Module):\n",
    "    def __init__(self, config: SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.self_attn = SiglipAttention(config)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps = config.layer_norm_eps)\n",
    "        self.mlp = SiglipMLP(config)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps = config.layer_norm_eps)\n",
    "\n",
    "    def forward(self,hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        # residual: [Batch_Size, Num_Patches, Embed_Dim]\n",
    "        residual = hidden_states\n",
    "        # [Batch_Size, Num_Patches, Embed_Dim] -> [Batch_Size, Num_Patches, Embed_Dim]\n",
    "        hidden_states = self.layer_norm1(hidden_states)\n",
    "        # [Batch_Size, Num_Patches, Embed_Dim] -> [Batch_Size, Num_Patches, Embed_Dim]\n",
    "        hidden_states, _ = self.self_attn(hidden_states=hidden_states)\n",
    "        # [Batch_Size, Num_Patches, Embed_Dim]\n",
    "        hidden_states = residual + hidden_states\n",
    "        # residual: [Batch_Size, Num_Patches, Embed_Dim] \n",
    "        residual = hidden_states\n",
    "        # [Batch_Size, Num_Patches, Embed_Dim] -> [Batch_Size, Num_Patches, Embed_Dim]\n",
    "        hidden_states = self.layer_norm2(hidden_states)\n",
    "        # [Batch_Size, Num_Patches, Embed_Dim] -> [Batch_Size, Num_Patches, Embed_Dim]\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        # [Batch_Size, Num_Patches, Embed_Dim]\n",
    "        hidden_states = residual + hidden_states\n",
    "        \n",
    "        return hidden_states\n",
    "\n",
    "class SiglipEncoder(nn.Module):\n",
    "    def __init__(self, config : SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layers = nn.ModuleList(\n",
    "            [SiglipEncoderLayer(config) for _ in range (config.num_hidden_layers)]\n",
    "        )\n",
    "\n",
    "        def forward(self, inputs_embeds: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "            #input_embeds : [Batch_Size, Num_Patches, Embed_Dim]\n",
    "            hidden_states = inputs_embeds\n",
    "\n",
    "            for encoder_layer in self.layers:\n",
    "                # [Batch_Size, Num_Patches, Embed_Dim] -> [Batch_Size, Num_Patches, Embed_Dim]\n",
    "                hidden_states = encoder_layer(hidden_states)\n",
    "\n",
    "            return hidden_states\n",
    "\n",
    "\n",
    "class SiglipVisionTransformer(nn.Module):\n",
    "    def __init__(self, config: SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config \n",
    "        embed_dim = config.hidden_size\n",
    "\n",
    "        self.embeddings = SiglipVisionEmbeddings(config)\n",
    "        self.encoder = SiglipEncoder(config)\n",
    "        self.post_layernorm = nn.LayerNorm(embed_dim, eps = config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n",
    "        # pixel_values : [Batch_Size, Channels, Height, Width] -> [Batch_Size, Num_Patches, Embed_Dim]\n",
    "        hidden_states = self.embeddings(pixel_values)\n",
    "        last_hidden_state = self.encoder(input_embeds = hidden_states)\n",
    "        last_hidden_state = self.post_layernorm(last_hidden_state)\n",
    "\n",
    "        return last_hidden_state\n",
    "\n",
    "class SiglipVisionModel(nn.Module):\n",
    "    def __init__(self, config:SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.vision_model = SiglipVisionTransformer(config)\n",
    "\n",
    "    def forward(self,pixel_values) -> Tuple:\n",
    "        # [Batch_Size, Channels, Height, Width] -> [Batch_Size, Num_Patches, Embed_Dim]\n",
    "        return self.vision_model(pixel_values = pixel_values)\n",
    "         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing PaliGemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Union, Tuple, Iterable\n",
    "import numpy as np \n",
    "from PIL import Image \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_STANDARD_MEAN = [0.5, 0.5, 0.5]\n",
    "IMAGENET_STANDARD_STD = [0.5, 0.5, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_image_tokens_to_prompt(prefix_prompt, bos_token, image_seq_len, image_token):\n",
    "    # Quoting from the blog (https://huggingface.co/blog/paligemma#detailed-inference-process):\n",
    "    #   The input text is tokenized normally.\n",
    "    #   A <bos> token is added at the beginning, and an additional newline token (\\n) is appended.\n",
    "    #   This newline token is an essential part of the input prompt the model was trained with, so adding it explicitly ensures it's always there.\n",
    "    #   The tokenized text is also prefixed with a fixed number of <image> tokens.\n",
    "    # NOTE: from the paper it looks like the `\\n` should be tokenized separately, but in the HF implementation this is not done.\n",
    "    #       ref to HF implementation: https://github.com/huggingface/transformers/blob/7f79a97399bb52aad8460e1da2f36577d5dccfed/src/transformers/models/paligemma/processing_paligemma.py#L55-L73\n",
    "    return f\"{image_token * image_seq_len}{bos_token}{prefix_prompt}\\n\"\n",
    "\n",
    "\n",
    "\n",
    "def rescale(\n",
    "    image: np.ndarray, scale: float, dtype: np.dtype = np.float32\n",
    ") -> np.ndarray:\n",
    "    rescaled_image = image * scale\n",
    "    rescaled_image = rescaled_image.astype(dtype)\n",
    "    return rescaled_image\n",
    "\n",
    "\n",
    "def resize(\n",
    "    image: Image,\n",
    "    size: Tuple[int, int],\n",
    "    resample: Image.Resampling = None,\n",
    "    reducing_gap: Optional[int] = None,\n",
    ") -> np.ndarray:\n",
    "    height, width = size\n",
    "    resized_image = image.resize(\n",
    "        (width, height), resample=resample, reducing_gap=reducing_gap\n",
    "    )\n",
    "    return resized_image\n",
    "\n",
    "\n",
    "def normalize(\n",
    "    image: np.ndarray,\n",
    "    mean: Union[float, Iterable[float]],\n",
    "    std: Union[float, Iterable[float]],\n",
    ") -> np.ndarray:\n",
    "    mean = np.array(mean, dtype=image.dtype)\n",
    "    std = np.array(std, dtype=image.dtype)\n",
    "    image = (image - mean) / std\n",
    "    return image\n",
    "\n",
    "def process_images(\n",
    "    images: List[Image.Image],\n",
    "    size: Dict[str, int] = None,\n",
    "    resample: Image.Resampling = None,\n",
    "    rescale_factor: float = None,\n",
    "    image_mean: Optional[Union[float, List[float]]] = None,\n",
    "    image_std: Optional[Union[float, List[float]]] = None,\n",
    ") -> List[np.ndarray]:\n",
    "    height, width = size[0], size[1]\n",
    "    images = [\n",
    "        resize(image=image, size=(height, width), resample=resample) for image in images\n",
    "    ]\n",
    "    # Convert each image to a numpy array\n",
    "    images = [np.array(image) for image in images]\n",
    "    # Rescale the pixel values to be in the range [0, 1]\n",
    "    images = [rescale(image, scale=rescale_factor) for image in images]\n",
    "    # Normalize the images to have mean 0 and standard deviation 1\n",
    "    images = [normalize(image, mean=image_mean, std=image_std) for image in images]\n",
    "    # Move the channel dimension to the first dimension. The model expects images in the format [Channel, Height, Width]\n",
    "    images = [image.transpose(2, 0, 1) for image in images]\n",
    "    return images\n",
    "\n",
    "\n",
    "class PaliGemmaProcessor:\n",
    "\n",
    "    IMAGE_TOKEN = \"<image>\"\n",
    "\n",
    "    def __init__(self, tokenizer, num_image_tokens: int, image_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_seq_length = num_image_tokens\n",
    "        self.image_size = image_size \n",
    "\n",
    "        tokens_to_add = {\"additional_special_tokens\" : [self.IMAGE_TOKEN]}\n",
    "        tokenizer.add_special_tokens(tokens_to_add)\n",
    "        EXTRA_TOKENS = [\n",
    "            f\"<loc{i:04d}>\" for i in range(1023)\n",
    "        ] # These tokens are used for object detection (bounding boxes)\n",
    "        EXTRA_TOKENS += [\n",
    "            f\"<seg{i:03d}>\" for i in range(128)\n",
    "        ] # These are tokens used for object segmentation \n",
    "        tokenizer.add_tokens(EXTRA_TOKENS)\n",
    "        self.image_token_id = tokenizer.convert_tokens_to_ids(self.IMAGE_TOKEN)\n",
    "\n",
    "        # We will add BOS and EOS tokens ourselves \n",
    "        tokenizer.add_bos_token = False\n",
    "        tokenizer.add_eos_token = False\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __call__(\n",
    "            self,\n",
    "            text: List[str],\n",
    "            images: List[Image.Image],\n",
    "            padding: str = \"longest\",\n",
    "            truncation: bool = True,\n",
    "    ) -> dict : \n",
    "        assert len(images) == 1 and len(text) ==1 , f\"Received {len(images)} images for {len(text)} prompts\"\n",
    "\n",
    "        pixel_values = process_images(\n",
    "            images,\n",
    "            size = (self.image_size, self.image_size),\n",
    "            resample = Image.Resampling.BICUBIC,\n",
    "            rescale_factor = 1 / 255.0,\n",
    "            image_mean = IMAGENET_STANDARD_MEAN,\n",
    "            image_std = IMAGENET_STANDARD_STD,\n",
    "        )\n",
    "\n",
    "        # Convert the list of numpy arrays into a single numpy array of shape [Batch_Size, Channel, Height, Width]\n",
    "        pixel_values = np.stack(pixel_values, axis = 0)\n",
    "        #Conver the numpy array to a PyTorch tensor \n",
    "        pixel_values = torch.tensor(pixel_values)\n",
    "\n",
    "        #Prepend a 'self.image_seq_length' number of image tokens to the prompt\n",
    "\n",
    "        input_strings = [\n",
    "            add_image_tokens_to_prompt(\n",
    "                prefix_prompt = prompt,\n",
    "                bos_token = self.tokenizer.bos_token,\n",
    "                image_seq_len = self.image_seq_length,\n",
    "                image_token = self.IMAGE_TOKEN,\n",
    "            )\n",
    "            for prompt in text\n",
    "        ]\n",
    "\n",
    "        # Returns the input_ids and attention_mask as PyTorch tensors \n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            input_strings,\n",
    "            return_tensors = \"pt\",\n",
    "            padding = padding,\n",
    "            truncation = truncation,\n",
    "        )\n",
    "\n",
    "        return_data = {\"pixel_values\": pixel_values, **inputs}\n",
    "\n",
    "        return return_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from typing import Optional, Tuple, List\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import math\n",
    "#from modeling_siglip import SiglipVisionConfig, SiglipVisionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from typing import Optional, Tuple, List\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import math\n",
    "#from modeling_siglip import SiglipVisionConfig, SiglipVisionModel\n",
    "\n",
    "class KVCache():\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.key_cache: List[torch.Tensor] = []\n",
    "        self.value_cache: List[torch.Tensor] = []\n",
    "    \n",
    "    def num_items(self) -> int:\n",
    "        if len(self.key_cache) == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            # The shape of the key_cache is [Batch_Size, Num_Heads_KV, Seq_Len, Head_Dim]\n",
    "            return self.key_cache[0].shape[-2]\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        key_states: torch.Tensor,\n",
    "        value_states: torch.Tensor,\n",
    "        layer_idx: int,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        if len(self.key_cache) <= layer_idx:\n",
    "            # If we never added anything to the KV-Cache of this layer, let's create it.\n",
    "            self.key_cache.append(key_states)\n",
    "            self.value_cache.append(value_states)\n",
    "        else:\n",
    "            # each tensor has shape: [Batch_Size, Num_Heads_KV, Seq_Len, Head_Dim]\n",
    "            self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)\n",
    "            self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)\n",
    "\n",
    "        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n",
    "\n",
    "class GemmaConfig():\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        hidden_size,\n",
    "        intermediate_size,\n",
    "        num_hidden_layers,\n",
    "        num_attention_heads,\n",
    "        num_key_value_heads,\n",
    "        head_dim=256,\n",
    "        max_position_embeddings=8192,\n",
    "        rms_norm_eps=1e-6,\n",
    "        rope_theta=10000.0,\n",
    "        attention_bias=False,\n",
    "        attention_dropout=0.0,\n",
    "        pad_token_id=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.num_key_value_heads = num_key_value_heads\n",
    "        self.rms_norm_eps = rms_norm_eps\n",
    "        self.rope_theta = rope_theta\n",
    "        self.attention_bias = attention_bias\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "class PaliGemmaConfig():\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vision_config=None,\n",
    "        text_config=None,\n",
    "        ignore_index=-100,\n",
    "        image_token_index=256000,\n",
    "        vocab_size=257152,\n",
    "        projection_dim=2048,\n",
    "        hidden_size=2048,\n",
    "        pad_token_id=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.ignore_index = ignore_index\n",
    "        self.image_token_index = image_token_index\n",
    "        self.vocab_size = vocab_size\n",
    "        self.projection_dim = projection_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vision_config = vision_config\n",
    "        self.is_encoder_decoder = False\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "        self.vision_config = SiglipVisionConfig(**vision_config)\n",
    "        self.text_config = text_config\n",
    "\n",
    "        self.text_config = GemmaConfig(**text_config, pad_token_id=pad_token_id)\n",
    "        self.vocab_size = self.text_config.vocab_size\n",
    "\n",
    "        self.text_config.num_image_tokens = (self.vision_config.image_size // self.vision_config.patch_size) ** 2\n",
    "        self.vision_config.projection_dim = projection_dim\n",
    "\n",
    "\n",
    "class GemmaRMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.zeros(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float())\n",
    "        output = output * (1.0 + self.weight.float())\n",
    "        return output.type_as(x)\n",
    "\n",
    "class GemmaRotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim \n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.base = base\n",
    "\n",
    "        \n",
    "        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float() / self.dim))\n",
    "        self.register_buffer(\"inv_freq\", tensor=inv_freq, persistent=False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x, position_ids, seq_len=None):\n",
    "        self.inv_freq.to(x.device)\n",
    "        # Copy the inv_freq tensor for batch in the sequence\n",
    "        # inv_freq_expanded: [Batch_Size, Head_Dim // 2, 1]\n",
    "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n",
    "        # position_ids_expanded: [Batch_Size, 1, Seq_Len]\n",
    "        position_ids_expanded = position_ids[:, None, :].float()\n",
    "        device_type = x.device.type\n",
    "        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n",
    "        with torch.autocast(device_type=device_type, enabled=False):\n",
    "            # freqs: [Batch_Size, Head_Dim // 2, 1] @ [Batch_Size, 1, Seq_Len] --> [Batch_Size, Seq_Len, Head_Dim // 2]\n",
    "            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
    "            # emb: [Batch_Size, Seq_Len, Head_Dim]\n",
    "            emb = torch.cat((freqs, freqs), dim=-1)\n",
    "            cos = emb.cos()\n",
    "            sin = emb.sin()\n",
    "        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n",
    "\n",
    "\n",
    "def rotate_half(x):\n",
    "    x1 = x[..., : x.shape[-1] // 2] \n",
    "    x2 = x[..., x.shape[-1] // 2 :] \n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, unsqueeze_dim=1):\n",
    "    cos = cos.unsqueeze(unsqueeze_dim) \n",
    "    sin = sin.unsqueeze(unsqueeze_dim) \n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "\n",
    "class GemmaMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.down_proj(nn.functional.gelu(self.gate_proj(x), approximate=\"tanh\") * self.up_proj(x))\n",
    "\n",
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "\n",
    "class GemmaAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config: GemmaConfig, layer_idx: Optional[int] = None):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "\n",
    "        self.attention_dropout = config.attention_dropout\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = config.head_dim\n",
    "        self.num_key_value_heads = config.num_key_value_heads\n",
    "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
    "        self.max_position_embeddings = config.max_position_embeddings\n",
    "        self.rope_theta = config.rope_theta\n",
    "        self.is_causal = True\n",
    "\n",
    "        assert self.hidden_size % self.num_heads == 0            \n",
    "\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
    "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n",
    "        self.rotary_emb = GemmaRotaryEmbedding(\n",
    "            self.head_dim,\n",
    "            max_position_embeddings=self.max_position_embeddings,\n",
    "            base=self.rope_theta,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        kv_cache: Optional[KVCache] = None,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        bsz, q_len, _ = hidden_states.size() # [Batch_Size, Seq_Len, Hidden_Size]\n",
    "        # [Batch_Size, Seq_Len, Num_Heads_Q * Head_Dim]\n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        # [Batch_Size, Seq_Len, Num_Heads_KV * Head_Dim]\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        # [Batch_Size, Seq_Len, Num_Heads_KV * Head_Dim]\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "        # [Batch_Size, Num_Heads_Q, Seq_Len, Head_Dim]\n",
    "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        # [Batch_Size, Num_Heads_KV, Seq_Len, Head_Dim]\n",
    "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        # [Batch_Size, Num_Heads_KV, Seq_Len, Head_Dim]\n",
    "        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # [Batch_Size, Seq_Len, Head_Dim], [Batch_Size, Seq_Len, Head_Dim]\n",
    "        cos, sin = self.rotary_emb(value_states, position_ids, seq_len=None)\n",
    "        # [Batch_Size, Num_Heads_Q, Seq_Len, Head_Dim], [Batch_Size, Num_Heads_KV, Seq_Len, Head_Dim]\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "        if kv_cache is not None:\n",
    "            key_states, value_states = kv_cache.update(key_states, value_states, self.layer_idx)\n",
    "\n",
    "        # Repeat the key and values to match the number of heads of the query\n",
    "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
    "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
    "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        assert attention_mask is not None\n",
    "        attn_weights = attn_weights + attention_mask\n",
    "\n",
    "        # Apply the softmax\n",
    "        # [Batch_Size, Num_Heads_Q, Seq_Len_Q, Seq_Len_KV]\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "        # Apply the dropout\n",
    "        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n",
    "        # Multiply by the values. [Batch_Size, Num_Heads_Q, Seq_Len_Q, Seq_Len_KV] x [Batch_Size, Num_Heads_KV, Seq_Len_KV, Head_Dim] -> [Batch_Size, Num_Heads_Q, Seq_Len_Q, Head_Dim]\n",
    "        attn_output = torch.matmul(attn_weights, value_states)\n",
    "\n",
    "        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n",
    "            raise ValueError(\n",
    "                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n",
    "                f\" {attn_output.size()}\"\n",
    "            )\n",
    "        # Make sure the sequence length is the second dimension. # [Batch_Size, Num_Heads_Q, Seq_Len_Q, Head_Dim] -> [Batch_Size, Seq_Len_Q, Num_Heads_Q, Head_Dim]\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        # Concatenate all the heads together. [Batch_Size, Seq_Len_Q, Num_Heads_Q, Head_Dim] -> [Batch_Size, Seq_Len_Q, Num_Heads_Q * Head_Dim]\n",
    "        attn_output = attn_output.view(bsz, q_len, -1)\n",
    "        # Multiply by W_o. [Batch_Size, Seq_Len_Q, Hidden_Size]\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        return attn_output, attn_weights\n",
    "\n",
    "class GemmaDecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, config: GemmaConfig, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        self.self_attn = GemmaAttention(config=config, layer_idx=layer_idx)\n",
    "\n",
    "        self.mlp = GemmaMLP(config)\n",
    "        self.input_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        kv_cache: Optional[KVCache] = None,\n",
    "    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
    "        residual = hidden_states\n",
    "        # [Batch_Size, Seq_Len, Hidden_Size]\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "\n",
    "        # [Batch_Size, Seq_Len, Hidden_Size]\n",
    "        hidden_states, _, = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            kv_cache=kv_cache,\n",
    "        )\n",
    "        # [Batch_Size, Seq_Len, Hidden_Size]\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        # [Batch_Size, Seq_Len, Hidden_Size]\n",
    "        residual = hidden_states\n",
    "        # [Batch_Size, Seq_Len, Hidden_Size]\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        # [Batch_Size, Seq_Len, Hidden_Size]\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        # [Batch_Size, Seq_Len, Hidden_Size]\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "class GemmaModel(nn.Module):\n",
    "\n",
    "    def __init__(self, config: GemmaConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [GemmaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
    "        )\n",
    "        self.norm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embed_tokens\n",
    "\n",
    "    # Ignore copy\n",
    "    def forward(\n",
    "        self,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        kv_cache: Optional[KVCache] = None,\n",
    "    ) -> torch.FloatTensor:\n",
    "        # [Batch_Size, Seq_Len, Hidden_Size]\n",
    "        hidden_states = inputs_embeds\n",
    "        # [Batch_Size, Seq_Len, Hidden_Size]\n",
    "        normalizer = torch.tensor(self.config.hidden_size**0.5, dtype=hidden_states.dtype)\n",
    "        hidden_states = hidden_states * normalizer\n",
    "\n",
    "        for decoder_layer in self.layers:\n",
    "            # [Batch_Size, Seq_Len, Hidden_Size]\n",
    "            hidden_states = decoder_layer(\n",
    "                hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                position_ids=position_ids,\n",
    "                kv_cache=kv_cache,\n",
    "            )\n",
    "\n",
    "        # [Batch_Size, Seq_Len, Hidden_Size]\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "\n",
    "        # [Batch_Size, Seq_Len, Hidden_Size]\n",
    "        return hidden_states\n",
    "\n",
    "class GemmaForCausalLM(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.model = GemmaModel(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.model.embed_tokens\n",
    "    \n",
    "    def tie_weights(self):\n",
    "        self.lm_head.weight = self.model.embed_tokens.weight\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        kv_cache: Optional[KVCache] = None,\n",
    "    ) -> Tuple:\n",
    "\n",
    "        # input_embeds: [Batch_Size, Seq_Len, Hidden_Size]\n",
    "        # outputs: [Batch_Size, Seq_Len, Hidden_Size]\n",
    "        outputs = self.model(\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            kv_cache=kv_cache,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs\n",
    "        logits = self.lm_head(hidden_states)\n",
    "        logits = logits.float()\n",
    "\n",
    "        return_data = {\n",
    "            \"logits\": logits,\n",
    "        }\n",
    "\n",
    "        if kv_cache is not None:\n",
    "            # Return the updated cache\n",
    "            return_data[\"kv_cache\"] = kv_cache\n",
    "\n",
    "        return return_data\n",
    "\n",
    "class PaliGemmaMultiModalProjector(nn.Module):\n",
    "    def __init__(self, config: PaliGemmaConfig):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(config.vision_config.hidden_size, config.vision_config.projection_dim, bias=True)\n",
    "\n",
    "    def forward(self, image_features):\n",
    "        # [Batch_Size, Num_Patches, Embed_Dim] -> [Batch_Size, Num_Patches, Projection_Dim]\n",
    "        hidden_states = self.linear(image_features)\n",
    "        return hidden_states\n",
    "\n",
    "class PaliGemmaForConditionalGeneration(nn.Module):\n",
    "    def __init__(self, config: PaliGemmaConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.vision_tower = SiglipVisionModel(config.vision_config)\n",
    "        self.multi_modal_projector = PaliGemmaMultiModalProjector(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        language_model = GemmaForCausalLM(config.text_config)\n",
    "        self.language_model = language_model\n",
    "\n",
    "        self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n",
    "\n",
    "    def tie_weights(self):\n",
    "        return self.language_model.tie_weights()\n",
    "\n",
    "    def _merge_input_ids_with_image_features(\n",
    "        self, image_features: torch.Tensor, inputs_embeds: torch.Tensor, input_ids: torch.Tensor, attention_mask: torch.Tensor, kv_cache: Optional[KVCache] = None\n",
    "    ):\n",
    "        _, _, embed_dim = image_features.shape\n",
    "        batch_size, sequence_length = input_ids.shape\n",
    "        dtype, device = inputs_embeds.dtype, inputs_embeds.device\n",
    "        # Shape: [Batch_Size, Seq_Len, Hidden_Size]\n",
    "        scaled_image_features = image_features / (self.config.hidden_size**0.5)\n",
    "    \n",
    "        # Combine the embeddings of the image tokens, the text tokens and mask out all the padding tokens.\n",
    "        final_embedding = torch.zeros(batch_size, sequence_length, embed_dim, dtype=inputs_embeds.dtype, device=inputs_embeds.device)\n",
    "        # Shape: [Batch_Size, Seq_Len]. True for text tokens\n",
    "        text_mask = (input_ids != self.config.image_token_index) & (input_ids != self.pad_token_id)\n",
    "        # Shape: [Batch_Size, Seq_Len]. True for image tokens\n",
    "        image_mask = input_ids == self.config.image_token_index\n",
    "        # Shape: [Batch_Size, Seq_Len]. True for padding tokens\n",
    "        pad_mask = input_ids == self.pad_token_id\n",
    "\n",
    "        # We need to expand the masks to the embedding dimension otherwise we can't use them in torch.where\n",
    "        text_mask_expanded = text_mask.unsqueeze(-1).expand(-1, -1, embed_dim)\n",
    "        pad_mask_expanded = pad_mask.unsqueeze(-1).expand(-1, -1, embed_dim)\n",
    "        image_mask_expanded = image_mask.unsqueeze(-1).expand(-1, -1, embed_dim)\n",
    "\n",
    "        # Add the text embeddings\n",
    "        final_embedding = torch.where(text_mask_expanded, inputs_embeds, final_embedding)\n",
    "        # Insert image embeddings. We can't use torch.where because the sequence length of scaled_image_features is not equal to the sequence length of the final embedding\n",
    "        final_embedding = final_embedding.masked_scatter(image_mask_expanded, scaled_image_features)\n",
    "        # Zero out padding tokens\n",
    "        final_embedding = torch.where(pad_mask_expanded, torch.zeros_like(final_embedding), final_embedding)\n",
    "\n",
    "        #### CREATE THE ATTENTION MASK ####\n",
    "\n",
    "        dtype, device = inputs_embeds.dtype, inputs_embeds.device\n",
    "        min_dtype = torch.finfo(dtype).min\n",
    "        q_len = inputs_embeds.shape[1]\n",
    "    \n",
    "        if kv_cache is None or kv_cache.num_items() == 0:\n",
    "            causal_mask = torch.full(\n",
    "                (batch_size, q_len, q_len), fill_value=0, dtype=dtype, device=device\n",
    "            )\n",
    "        else:\n",
    "            assert q_len == 1\n",
    "            kv_len = kv_cache.num_items() + q_len\n",
    "            causal_mask = torch.full(\n",
    "                (batch_size, q_len, kv_len), fill_value=0, dtype=dtype, device=device\n",
    "            )\n",
    "\n",
    "        causal_mask = causal_mask.unsqueeze(1)\n",
    "\n",
    "        if kv_cache is not None and kv_cache.num_items() > 0:\n",
    "            position_ids = attention_mask.cumsum(-1)[:, -1]\n",
    "            if position_ids.dim() == 1:\n",
    "                position_ids = position_ids.unsqueeze(0)\n",
    "        else:\n",
    "            position_ids = (attention_mask.cumsum(-1)).masked_fill_((attention_mask == 0), 1).to(device)\n",
    "\n",
    "        return final_embedding, causal_mask, position_ids\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        pixel_values: torch.FloatTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        kv_cache: Optional[KVCache] = None,\n",
    "    ) -> Tuple:\n",
    "\n",
    "        # Make sure the input is right-padded\n",
    "        assert torch.all(attention_mask == 1), \"The input cannot be padded\"\n",
    "\n",
    "        # 1. Extra the input embeddings\n",
    "        # shape: (Batch_Size, Seq_Len, Hidden_Size)\n",
    "        inputs_embeds = self.language_model.get_input_embeddings()(input_ids)\n",
    "\n",
    "        # 2. Merge text and images\n",
    "        # [Batch_Size, Channels, Height, Width] -> [Batch_Size, Num_Patches, Embed_Dim]\n",
    "        selected_image_feature = self.vision_tower(pixel_values.to(inputs_embeds.dtype))\n",
    "        # [Batch_Size, Num_Patches, Embed_Dim] -> [Batch_Size, Num_Patches, Hidden_Size]\n",
    "        image_features = self.multi_modal_projector(selected_image_feature)\n",
    "\n",
    "        # Merge the embeddings of the text tokens and the image tokens\n",
    "        inputs_embeds, attention_mask, position_ids = self._merge_input_ids_with_image_features(image_features, inputs_embeds, input_ids, attention_mask, kv_cache)\n",
    "        \n",
    "        outputs = self.language_model(\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            kv_cache=kv_cache,\n",
    "        )\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from modeling_gemma import PaliGemmaForConditionalGeneration, PaliGemmaConfig\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "import glob\n",
    "from safetensors import safe_open\n",
    "from typing import Tuple\n",
    "import os\n",
    "\n",
    "def load_hf_model(model_path: str, device: str) -> Tuple[PaliGemmaForConditionalGeneration, AutoTokenizer]:\n",
    "    # Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=\"right\")\n",
    "    assert tokenizer.padding_side == \"right\"\n",
    "\n",
    "    safetensors_files = glob.glob(os.path.join(model_path, \"*.safetensors\"))\n",
    "\n",
    "    tensors = {}\n",
    "    for safetensors_file in safetensors_files:\n",
    "        with safe_open(safetensors_file, framework=\"pt\", device=\"cpu\") as f:\n",
    "            for key in f.keys():\n",
    "                tensors[key] = f.get_tensor(key)\n",
    "\n",
    "    with open(os.path.join(model_path, \"config.json\"), \"r\") as f:\n",
    "        model_config_file = json.load(f)\n",
    "        config = PaliGemmaConfig(**model_config_file)\n",
    "\n",
    "    model = PaliGemmaForConditionalGeneration(config).to(device)\n",
    "\n",
    "    model.load_state_dict(tensors, strict=False)\n",
    "\n",
    "    model.tie_weights()\n",
    "\n",
    "    return (model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device in use:  cuda\n",
      "Loading model\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "None is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/progs/miniconda3/envs/visual_transformer/lib/python3.9/site-packages/huggingface_hub/utils/_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/progs/miniconda3/envs/visual_transformer/lib/python3.9/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/None/resolve/main/tokenizer_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m/opt/progs/miniconda3/envs/visual_transformer/lib/python3.9/site-packages/transformers/utils/hub.py:399\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 399\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/progs/miniconda3/envs/visual_transformer/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/progs/miniconda3/envs/visual_transformer/lib/python3.9/site-packages/huggingface_hub/file_download.py:862\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/progs/miniconda3/envs/visual_transformer/lib/python3.9/site-packages/huggingface_hub/file_download.py:969\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m--> 969\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/progs/miniconda3/envs/visual_transformer/lib/python3.9/site-packages/huggingface_hub/file_download.py:1484\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1482\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1483\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1484\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1486\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m/opt/progs/miniconda3/envs/visual_transformer/lib/python3.9/site-packages/huggingface_hub/file_download.py:1376\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1376\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m/opt/progs/miniconda3/envs/visual_transformer/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/progs/miniconda3/envs/visual_transformer/lib/python3.9/site-packages/huggingface_hub/file_download.py:1296\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1295\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1296\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1305\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m/opt/progs/miniconda3/envs/visual_transformer/lib/python3.9/site-packages/huggingface_hub/file_download.py:277\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 277\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/progs/miniconda3/envs/visual_transformer/lib/python3.9/site-packages/huggingface_hub/file_download.py:301\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    300\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 301\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/progs/miniconda3/envs/visual_transformer/lib/python3.9/site-packages/huggingface_hub/utils/_http.py:454\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    446\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m     )\n\u001b[0;32m--> 454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(RepositoryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-6759abb0-294b845b3f7b2d820c505fc2;d1a7849f-4943-4431-bb91-e320a9492896)\n\nRepository Not Found for url: https://huggingface.co/None/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 142\u001b[0m\n\u001b[1;32m    128\u001b[0m         test_inference(\n\u001b[1;32m    129\u001b[0m             model,\n\u001b[1;32m    130\u001b[0m             processor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    137\u001b[0m             do_sample,\n\u001b[1;32m    138\u001b[0m         )\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 142\u001b[0m     \u001b[43mfire\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/progs/miniconda3/envs/visual_transformer/lib/python3.9/site-packages/fire/core.py:143\u001b[0m, in \u001b[0;36mFire\u001b[0;34m(component, command, name, serialize)\u001b[0m\n\u001b[1;32m    140\u001b[0m   context\u001b[38;5;241m.\u001b[39mupdate(caller_globals)\n\u001b[1;32m    141\u001b[0m   context\u001b[38;5;241m.\u001b[39mupdate(caller_locals)\n\u001b[0;32m--> 143\u001b[0m component_trace \u001b[38;5;241m=\u001b[39m \u001b[43m_Fire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomponent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparsed_flag_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m component_trace\u001b[38;5;241m.\u001b[39mHasError():\n\u001b[1;32m    146\u001b[0m   _DisplayError(component_trace)\n",
      "File \u001b[0;32m/opt/progs/miniconda3/envs/visual_transformer/lib/python3.9/site-packages/fire/core.py:477\u001b[0m, in \u001b[0;36m_Fire\u001b[0;34m(component, args, parsed_flag_args, context, name)\u001b[0m\n\u001b[1;32m    474\u001b[0m is_class \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39misclass(component)\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 477\u001b[0m   component, remaining_args \u001b[38;5;241m=\u001b[39m \u001b[43m_CallAndUpdateTrace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcomponent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m      \u001b[49m\u001b[43mremaining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcomponent_trace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtreatment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclass\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mroutine\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomponent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    483\u001b[0m   handled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m FireError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[0;32m/opt/progs/miniconda3/envs/visual_transformer/lib/python3.9/site-packages/fire/core.py:693\u001b[0m, in \u001b[0;36m_CallAndUpdateTrace\u001b[0;34m(component, args, component_trace, treatment, target)\u001b[0m\n\u001b[1;32m    691\u001b[0m   component \u001b[38;5;241m=\u001b[39m loop\u001b[38;5;241m.\u001b[39mrun_until_complete(fn(\u001b[38;5;241m*\u001b[39mvarargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 693\u001b[0m   component \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvarargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m treatment \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    696\u001b[0m   action \u001b[38;5;241m=\u001b[39m trace\u001b[38;5;241m.\u001b[39mINSTANTIATED_CLASS\n",
      "Cell \u001b[0;32mIn[12], line 119\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(model_path, prompt, image_file_path, max_tokens_to_generate, temperature, top_p, do_sample, only_cpu)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDevice in use: \u001b[39m\u001b[38;5;124m\"\u001b[39m, device)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 119\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload_hf_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    122\u001b[0m num_image_tokens \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mvision_config\u001b[38;5;241m.\u001b[39mnum_image_tokens\n",
      "Cell \u001b[0;32mIn[11], line 11\u001b[0m, in \u001b[0;36mload_hf_model\u001b[0;34m(model_path, device)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_hf_model\u001b[39m(model_path: \u001b[38;5;28mstr\u001b[39m, device: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[PaliGemmaForConditionalGeneration, AutoTokenizer]:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Load the tokenizer\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mright\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mpadding_side \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m     safetensors_files \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m/opt/progs/miniconda3/envs/visual_transformer/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:817\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[0;32m--> 817\u001b[0m tokenizer_config \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokenizer_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[1;32m    819\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/progs/miniconda3/envs/visual_transformer/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:649\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    646\u001b[0m     token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[1;32m    648\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 649\u001b[0m resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    666\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/progs/miniconda3/envs/visual_transformer/lib/python3.9/site-packages/transformers/utils/hub.py:422\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    418\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    419\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    420\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 422\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    425\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    426\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    427\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    431\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    432\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    433\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: None is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import fire\n",
    "\n",
    "#from processing_paligemma import PaliGemmaProcessor\n",
    "#from modeling_gemma import KVCache, PaliGemmaForConditionalGeneration\n",
    "#from utils import load_hf_model\n",
    "\n",
    "\n",
    "def move_inputs_to_device(model_inputs: dict, device: str):\n",
    "    model_inputs = {k: v.to(device) for k, v in model_inputs.items()}\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def get_model_inputs(\n",
    "    processor: PaliGemmaProcessor, prompt: str, image_file_path: str, device: str\n",
    "):\n",
    "    image = Image.open(image_file_path)\n",
    "    images = [image]\n",
    "    prompts = [prompt]\n",
    "    model_inputs = processor(text=prompts, images=images)\n",
    "    model_inputs = move_inputs_to_device(model_inputs, device)\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def test_inference(\n",
    "    model: PaliGemmaForConditionalGeneration,\n",
    "    processor: PaliGemmaProcessor,\n",
    "    device: str,\n",
    "    prompt: str,\n",
    "    image_file_path: str,\n",
    "    max_tokens_to_generate: int,\n",
    "    temperature: float,\n",
    "    top_p: float,\n",
    "    do_sample: bool,\n",
    "):\n",
    "    model_inputs = get_model_inputs(processor, prompt, image_file_path, device)\n",
    "    input_ids = model_inputs[\"input_ids\"]\n",
    "    attention_mask = model_inputs[\"attention_mask\"]\n",
    "    pixel_values = model_inputs[\"pixel_values\"]\n",
    "\n",
    "    kv_cache = KVCache()\n",
    "\n",
    "    # Generate tokens until you see the stop token\n",
    "    stop_token = processor.tokenizer.eos_token_id\n",
    "    generated_tokens = []\n",
    "\n",
    "    for _ in range(max_tokens_to_generate):\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            pixel_values=pixel_values,\n",
    "            attention_mask=attention_mask,\n",
    "            kv_cache=kv_cache,\n",
    "        )\n",
    "        kv_cache = outputs[\"kv_cache\"]\n",
    "        next_token_logits = outputs[\"logits\"][:, -1, :]\n",
    "        if do_sample:\n",
    "            next_token_logits = torch.softmax(next_token_logits / temperature, dim=-1)\n",
    "            next_token = _sample_top_p(next_token_logits, top_p)\n",
    "        else:\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "        assert next_token.size() == (1, 1)\n",
    "        next_token = next_token.squeeze(0)  \n",
    "        generated_tokens.append(next_token)\n",
    "        if next_token.item() == stop_token:\n",
    "            break\n",
    "        input_ids = next_token.unsqueeze(-1)\n",
    "        attention_mask = torch.cat(\n",
    "            [attention_mask, torch.ones((1, 1), device=input_ids.device)], dim=-1\n",
    "        )\n",
    "\n",
    "    generated_tokens = torch.cat(generated_tokens, dim=-1)\n",
    "    # Decode the generated tokens\n",
    "    decoded = processor.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "    print(prompt + decoded)\n",
    "\n",
    "\n",
    "def _sample_top_p(probs: torch.Tensor, p: float):\n",
    "    # (B, vocab_size)\n",
    "    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "    # (B, vocab_size)\n",
    "    probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
    "    # (B, vocab_size)\n",
    "    # (Substracting \"probs_sort\" shifts the cumulative sum by 1 position to the right before masking)\n",
    "    mask = probs_sum - probs_sort > p\n",
    "    # Zero out all the probabilities of tokens that are not selected by the Top P\n",
    "    probs_sort[mask] = 0.0\n",
    "    # Redistribute the probabilities so that they sum up to 1.\n",
    "    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "    # Sample a token (its index) from the top p distribution\n",
    "    next_token = torch.multinomial(probs_sort, num_samples=1)\n",
    "    # Get the token position in the vocabulary corresponding to the sampled index\n",
    "    next_token = torch.gather(probs_idx, -1, next_token)\n",
    "    return next_token\n",
    "\n",
    "\n",
    "def main(\n",
    "    model_path: str = None,\n",
    "    prompt: str = None,\n",
    "    image_file_path: str = None,\n",
    "    max_tokens_to_generate: int = 100,\n",
    "    temperature: float = 0.8,\n",
    "    top_p: float = 0.9,\n",
    "    do_sample: bool = False,\n",
    "    only_cpu: bool = False,\n",
    "):\n",
    "    device = \"cpu\"\n",
    "\n",
    "    if not only_cpu:\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda\"\n",
    "        elif torch.backends.mps.is_available():\n",
    "            device = \"mps\"\n",
    "\n",
    "    print(\"Device in use: \", device)\n",
    "\n",
    "    print(f\"Loading model\")\n",
    "    model, tokenizer = load_hf_model(model_path, device)\n",
    "    model = model.to(device).eval()\n",
    "\n",
    "    num_image_tokens = model.config.vision_config.num_image_tokens\n",
    "    image_size = model.config.vision_config.image_size\n",
    "    processor = PaliGemmaProcessor(tokenizer, num_image_tokens, image_size)\n",
    "\n",
    "    print(\"Running inference\")\n",
    "    with torch.no_grad():\n",
    "        test_inference(\n",
    "            model,\n",
    "            processor,\n",
    "            device,\n",
    "            prompt,\n",
    "            image_file_path,\n",
    "            max_tokens_to_generate,\n",
    "            temperature,\n",
    "            top_p,\n",
    "            do_sample,\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(main)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path=\"/home/luk_lab/Desktop/Luk Lab - AI/projects/thesis/Visual_Transformers/paligemma-3b-pt-224\"\n",
    "image_file_path=\"/home/luk_lab/Desktop/Luk Lab - AI/projects/thesis/Visual_Transformers/white.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(f\"Model path does not exist: {model_path}\")\n",
    "if not os.path.exists(image_file_path):\n",
    "    raise FileNotFoundError(f\"Image file does not exist: {image_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -c \"from transformers import AutoTokenizer; AutoTokenizer.from_pretrained('/home/luk_lab/Desktop/Luk Lab - AI/projects/thesis/Visual_Transformers/paligemma-3b-pt-224')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = None\n",
    "def load_hf_model(model_path):\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=\"right\")\n",
    "        print(\"Model Path is:\", model_path)\n",
    "        print(type(tokenizer))\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading tokenizer from {model_path}: {e}\")\n",
    "        raise\n",
    "    # Load model logic here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Path is: /home/luk_lab/Desktop/Luk Lab - AI/projects/thesis/Visual_Transformers/paligemma-3b-pt-224\n",
      "<class 'transformers.models.gemma.tokenization_gemma_fast.GemmaTokenizerFast'>\n"
     ]
    }
   ],
   "source": [
    "load_hf_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visual_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
